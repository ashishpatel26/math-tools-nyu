{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required modules\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.optimize import minimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " ! svn export https://github.com/LabForComputationalVision/bias_free_denoising/trunk/data/Train400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_image_data = 'Train400'\n",
    "path_to_saved_models = './image_wavelet/saved_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_source = glob.glob(os.path.join(path_to_image_data, '*.png'))\n",
    "files_source = np.sort(files_source)\n",
    "list_of_image_path = files_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_to_256 = True\n",
    "n_data = len(list_of_image_path)\n",
    "\n",
    "data_list = [None]*n_data;\n",
    "for i in range(n_data):\n",
    "    f = list_of_image_path[i];\n",
    "    Img = cv2.imread(f)\n",
    "    Img = np.float32(Img[:,:,0])/255.\n",
    "    if resize_to_256:\n",
    "        Img = cv2.resize(Img, (256, 256)) \n",
    "        data_list[i] = Img;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a 50 examples subset to reduce computation\n",
    "random_batch = np.random.choice(range(len(data_list[:50])), 1)\n",
    "data_list = np.array(data_list)[random_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Wiener Filter Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(data, noise_std = 0.1, mode='S', max_noise = 5e-1):\n",
    "    noise = np.random.randn(*data.shape);\n",
    "    if mode == 'B':\n",
    "        n = noise.shape[0];\n",
    "        noise_tensor_array = max_noise * np.random.rand(n);\n",
    "        for i in range(n):\n",
    "            noise[i] = noise[i] * noise_tensor_array[i];\n",
    "    else:\n",
    "        noise = noise * noise_std;\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_wiener_loss(dataset, noise_std, wiener_star, fft_sample= (180, 180)):\n",
    "    mean_loss = 0;\n",
    "    mean_psnr = 0;\n",
    "    for source in dataset:\n",
    "        noisy = source + get_noise(source, noise_std=noise_std)\n",
    "\n",
    "        source_fft = np.fft.fftshift( np.fft.fft2(source, s = fft_sample) );\n",
    "        noisy_fft = np.fft.fftshift( np.fft.fft2(noisy, s = fft_sample) );\n",
    "        denoised_fft = noisy_fft * wiener_star;\n",
    "\n",
    "        source_ifft = np.clip( np.real( np.fft.ifft2( np.fft.ifftshift(source_fft), s= fft_sample) ), 0., 1. );\n",
    "        noisy_ifft = np.clip( np.real(np.fft.ifft2( np.fft.ifftshift( noisy_fft), s = fft_sample)), 0., 1. );\n",
    "        denoised_ifft = np.clip( np.real( np.fft.ifft2( np.fft.ifftshift(denoised_fft), s = fft_sample)), 0., 1. );\n",
    "        \n",
    "        mean_loss += np.sum((denoised_ifft - source_ifft)**2);\n",
    "    \n",
    "    return mean_loss/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_wiener_from_data(train_dataset, noise_std, fft_shift = True, fft_sample = (256, 256),  n = 10):\n",
    "    # Args: \n",
    "    #  train_dataset: data used to calculate wiener filter\n",
    "    #  noise std: standard deviation to control noise level\n",
    "    #  fftshift: shift the zero-frequency component to the center of the spectrum\n",
    "    #  n: number of epochs\n",
    "    \n",
    "    # Pelase fill in the code\n",
    "    mean_coeff = np.fft.fft2(train_dataset[0], s = fft_sample) * 0;\n",
    "    for i in range(n):\n",
    "        for x in train_dataset:\n",
    "            'Fill here to get the sample wiener filter'\n",
    "            \n",
    "\n",
    "    mean_coeff /= n*len(train_dataset)\n",
    "    if fft_shift:\n",
    "        return np.fft.fftshift(mean_coeff)\n",
    "    else:\n",
    "        return mean_coeff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For different noise level, produce calculated wiener filter\n",
    "noise_std_array = [float(10)/255, float(25)/255, float(55)/255];\n",
    "wiener_star = {}\n",
    "for noise_std in noise_std_array:\n",
    "    print('noise level: ', noise_std)\n",
    "    wiener_star[noise_std] = theoretical_wiener_from_data(data_list, noise_std, fft_shift = True, fft_sample = (256, 256),  n = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain loss values of the the winer filter on different noise level\n",
    "for noise_std in noise_std_array:\n",
    "    print('noise std: ', noise_std)\n",
    "    wiener_star_loss = best_wiener_loss(data_list, noise_std, wiener_star[noise_std], fft_sample = (256, 256))\n",
    "    print('Loss: ', wiener_star_loss)\n",
    "    print('='*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we focus on a linear model to perform denoising.\n",
    "\n",
    "The model is parmetrized by a filter $w$ and optimized by the loss function $\\frac{1}{n} ||w*x_{noisy} - x_{clean} ||^2$\n",
    "\n",
    "You will need the function: convolve2d(x_noisy, w, mode='same', boundary = 'wrap') to compute the convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_points = len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(w, clean_dataset, noisy_dataset):\n",
    "    # Args:\n",
    "    #  w: filter with size usually 3*3 or 5*5\n",
    "    #  clean version of dataset\n",
    "    #  noisy version of dataset\n",
    "    loss = 0;\n",
    "    for (source, noisy) in zip(clean_dataset, noisy_dataset):\n",
    "        w_2d = w.reshape( int(np.sqrt(len(w))), int(np.sqrt(len(w))) ) # reshapre filter to 2D\n",
    "        \"Fill here to comupte the loss for each instance and sum them up\"\n",
    "        \n",
    "    return loss/len(clean_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the filter to 5*5\n",
    "w_len = 5*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate initial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_dict = {}\n",
    "noisy_dataset = {}\n",
    "for noise_std in noise_std_array:\n",
    "    noisy_dataset[noise_std] = [source + get_noise(source, noise_std=noise_std) for source in data_list]\n",
    "    w0_dict[noise_std] = np.random.randn(w_len)*0.001\n",
    "    least_squre_loss = loss(w0_dict[noise_std], data_list[:n_data_points], noisy_dataset[noise_std][:n_data_points])\n",
    "    print('Loss: ', least_squre_loss)\n",
    "    print('='*50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model\n",
    "w = {}\n",
    "for noise_std in noise_std_array:\n",
    "    print('noise std: ', noise_std)\n",
    "    \n",
    "    w0 = w0_dict[noise_std]\n",
    "    res = minimize(loss, w0, args = (data_list[:n_data_points], noisy_dataset[noise_std][:n_data_points]), method='SLSQP', tol=1e-6)\n",
    "    w[noise_std] = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please Compute the loss value on the optimized $w$, how does the loss value changes omparing with the random initialization, and with the loss of the theoretical wiener filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple nonlinear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple nonlinear model as a composition of convolution, ReLU, convolution layers, and optimize this nonlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_relu_conv(big_w, noisy):\n",
    "    #big_w: a big filter without reshape\n",
    "    #noisy: noisy data\n",
    "    #return the output of the model\n",
    "    n = len(big_w)\n",
    "    w1 = big_w[:n//2].reshape( int(np.sqrt(n//2)), int(np.sqrt(n//2))) # filter for the first convolution\n",
    "    w2 =big_w[n//2:].reshape(int(np.sqrt(n//2)), int(np.sqrt(n//2))) # filter for the second convolution\n",
    "    \n",
    "    \"fill here to implement the nonlinear model (for ReLU, you can implement it as f(x) = max(0,x), refer https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\"\n",
    "    \n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cnn(w, clean_dataset, noisy_dataset):\n",
    "    mean_loss = 0;\n",
    "    \n",
    "    for (source, noisy) in zip(clean_dataset, noisy_dataset):\n",
    "        \n",
    "        mean_loss +=  np.sum((conv_relu_conv(w, noisy ) - source)**2);\n",
    "        \n",
    "    return mean_loss/len(clean_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_len = 5*5*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate initial loss\n",
    "w0_dict = {}\n",
    "noisy_dataset = {}\n",
    "for noise_std in noise_std_array:\n",
    "    noisy_dataset[noise_std] = [source + get_noise(source, noise_std=noise_std) for source in data_list]\n",
    "    w0_dict[noise_std] = np.random.randn(w_len)*0.001\n",
    "    least_squre_loss = loss_cnn(w0_dict[noise_std], data_list[:n_data_points], noisy_dataset[noise_std][:n_data_points])\n",
    "    print('Loss: ', least_squre_loss)\n",
    "    print('='*50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the model\n",
    "w = {}\n",
    "for noise_std in noise_std_array:\n",
    "    print('noise std: ', noise_std)\n",
    "    \n",
    "    w0 = w0_dict[noise_std]\n",
    "    res = minimize(loss_cnn, w0, args = (data_list[:n_data_points], noisy_dataset[noise_std][:n_data_points]), method='SLSQP', tol=1e-6)\n",
    "    w[noise_std] = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for noise_std in noise_std_array:\n",
    "    print('noise std: ', noise_std)\n",
    "    least_squre_loss = loss_cnn(w[noise_std], data_list[:n_data_points], noisy_dataset[noise_std][:n_data_points])\n",
    "    print('Loss: ', least_squre_loss)\n",
    "\n",
    "    print('='*50 + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please Compute the loss value of this nonlinear model on the optimized $w$, how does the loss value change comparing with the random initialization, with the linear model and with the loss of the theoretical wiener filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
